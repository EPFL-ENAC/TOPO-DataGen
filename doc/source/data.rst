Data
====

All georeferenced data (used as input or generated during the
TOPO-DataGen process) are in webmarcator `EPSG:3857 <https://epsg.io/3857>`__.

Input data
----------

Classified point cloud
~~~~~~~~~~~~~~~~~~~~~~

**Description** : models all natural and man-made objects of the surface
in the form of a classified point cloud. These high-accuracy and high
spatial density data are collected by airborne LiDAR.

**Format** : las

**Data sample** : `SURFACE3D demo <https://raw.githubusercontent.com/EPFL-ENAC/TOPO-DataGen/main/data_preprocess/demo/surface3d.csv>`__

|image0|

--------------

Digital surface model
~~~~~~~~~~~~~~~~~~~~~

**Description** : digital surface model (DSM) which represents the
earthâ€™s surface including visible and permanent landscape elements such
as soil, natural cover, and all sorts of constructive work with the
exception of power lines and masts.

**Format** : tif

**Data sample** : `SURFACE3D Raster demo <https://raw.githubusercontent.com/EPFL-ENAC/TOPO-DataGen/main/data_preprocess/demo/surface3d-raster.csv>`__

|image1|

--------------

Orthophoto mosaic
~~~~~~~~~~~~~~~~~

**Description** : Digital color aerial photographs.

**Format** : tif

**Data sample** : `SWISSIMAGE 10cm demo <https://raw.githubusercontent.com/EPFL-ENAC/TOPO-DataGen/main/data_preprocess/demo/swissimage10.csv>`__\

|image2|





--------------

Drone footage (optional)
~~~~~~~~~~~~~~~~~~~~~~~~

**Description** : Real georeferenced images. These images are typically
take from a done and contain 6D pose parameters (longitude, latitude,
elevation, roll, pitch, yaw.

If provided, the position of these pictures will be used to define the
poses of the output products. It creates real-synthetics pairs that
share the same geolocalisation (6D position). However it is possible to
generate the output products without providing any drone footages. In
this case, 3D near-random position will be generated using Latin
Hypercube Sampling (LHS) method.

.. note::
   The data sample provided are taken from campaign made with a `Phantom4 drone <https://www.dji.com/ch>`__. Other drones may produce slightlydifferent pictures metadata (EXIF).

**Format** : JEPG

**Data sample** : `done footage <https://zenodo.org/record/7251570/files/drone_footages.zip?download=1>`__

|image3|

--------------

Output data
-----------

The following data are generated by the TOPO-DataGen process.

Synthetic RGB image
~~~~~~~~~~~~~~~~~~~

**Description** : RGB synthetic raster based on the 3D textured model
rendered by Cesium JS

**Format** : png

|image4|

--------------

Scene coordinates
~~~~~~~~~~~~~~~~~

**Description** : Pixel-wise scene coordinates

**Format** : png

|image5|

--------------

Semantics label
~~~~~~~~~~~~~~~

**Description** : Define the nature of each pixel based on the
classified point cloud categories.

**Format** : png

|image6|

.. _section-1:

--------------

Depth
~~~~~

**Description** : Distance between the camera and the pixel

**Format** : png

|image7|

.. _section-2:

--------------

Surface normal vector
~~~~~~~~~~~~~~~~~~~~~

**Description** : Direction of the surface normal vector

**Format** : png

|image8|

.. _section-3:

--------------

2D/3D keypoints
~~~~~~~~~~~~~~~

**Description** : Keypoints are points of interest in an image that can
be used to compare images and perform tasks such as image alignment and
registration

**Format** : png

|image9|

.. |image0| image:: /_static/image_surface3d.jpg
.. |image1| image:: /_static/image_surface3d_raster.jpg
.. |image2| image:: /_static/image_SWISSIMAGE10.jpg
.. |image3| image:: /_static/drone_footage.png
.. |image4| image:: /_static/synthetic.png
.. |image5| image:: /_static/scene_coordinates.png
.. |image6| image:: /_static/semantics.png
.. |image7| image:: /_static/depth.png
.. |image8| image:: /_static/normal.png
.. |image9| image:: /_static/orb.png
