Data
====

All georeferenced data (used as input or generated during the
TOPO-DataGen process) are in `EPSG:4326 <https://epsg.io/4326>`__.

The `tutorial <tutorial.md>`__ illustrates the reprojection of national
coordinate system based data (swiss in this case) into global WG84
EPSG:4326. This process can be adapted to use oher source data.

Input data
----------

Classified point cloud
~~~~~~~~~~~~~~~~~~~~~~

**Description** : models all natural and man-made objects of the surface
in the form of a classified point cloud. These high-accuracy and high
spatial density data are collected by airborne LiDAR.

**Format** : las

**Data sample** : `swiss example -
swissSURFACE3D <https://www.swisstopo.admin.ch/en/geodata/height/surface3d.html>`__

|image0|

--------------

Digital surface model
~~~~~~~~~~~~~~~~~~~~~

**Description** : digital surface model (DSM) which represents the
earthâ€™s surface including visible and permanent landscape elements such
as soil, natural cover, and all sorts of constructive work with the
exception of power lines and masts.

**Format** : tif

**Data sample** : `swiss example - swissSURFACE3D
Raster <%5BswissSURFACE3D%20Raster%20-%20swisstopo%5D(https://www.swisstopo.admin.ch/en/geodata/height/surface3d-raster.html)>`__

|image1|

--------------

Orthophoto mosaic
~~~~~~~~~~~~~~~~~

**Description** : Digital color aerial photographs.

**Format** : tif

**Data sample** : `swiss example - SWISSIMAGE 10
cm <https://www.swisstopo.admin.ch/en/geodata/images/ortho/swissimage10.html>`__\ 

|image2|





--------------

Drone footage (optional)
~~~~~~~~~~~~~~~~~~~~~~~~

**Description** : Real georeferenced images. These images are typically
take from a done and contain 6D pose parameters (longitude, latitude,
elevation, roll, pitch, yaw.

If provided, the position of these pictures will be used to define the
poses of the output products. It creates real-synthetics pairs that
share the same geolocalisation (6D position). However it is possible to
generate the output products without providing any drone footages. In
this case, 3D near-random position will be generated using Latin
Hypercube Sampling (LHS) method.

.. note::
   The data sample provided are taken from campaign made with a `Phantom4 drone <https://www.dji.com/ch>`__. Other drones may produce slightlydifferent pictures metadata (EXIF).

**Format** : JEPG

**Data sample** : to do

|image3|

--------------

Output data
-----------

The following data are generated by the TOPO-DataGen process.

Synthetic RGB image
~~~~~~~~~~~~~~~~~~~

**Description** : RGB synthetic raster based on the 3D textured model
rendered by Cesium JS

**Format** : png

|image4|

--------------

Scene coordinates
~~~~~~~~~~~~~~~~~

**Description** : Pixel-wise scene coordinates

**Format** : To be defined

|image5|

--------------

Semantics label
~~~~~~~~~~~~~~~

**Description** : Define the nature of each pixel based on the
classified point cloud categories.

**Format** : To be defined

|image6|

.. _section-1:

--------------

Depth
~~~~~

**Description** : Distance between the camera and the pixel

**Format** : To be defined

|image7|

.. _section-2:

--------------

Surface normal vector
~~~~~~~~~~~~~~~~~~~~~

**Description** : Direction of the surface normal vector

**Format** : To be defined

|image8|

.. _section-3:

--------------

2D/3D keypoints
~~~~~~~~~~~~~~~

**Description** : Keypoints are points of interest in an image that can
be used to compare images and perform tasks such as image alignment and
registration

**Format** : To be defined

|image9|

.. |image0| image:: /_static/image_surface3d.jpg
.. |image1| image:: /_static/image_surface3d_raster.jpg
.. |image2| image:: /_static/image_SWISSIMAGE10.jpg
.. |image3| image:: /_static/drone_footage.png
.. |image4| image:: /_static/synthetic.png
.. |image5| image:: /_static/scene_coordinates.png
.. |image6| image:: /_static/semantics.png
.. |image7| image:: /_static/depth.png
.. |image8| image:: /_static/normal.png
.. |image9| image:: /_static/orb.png
